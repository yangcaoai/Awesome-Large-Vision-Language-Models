# Awesome-Large-Vision-Language-Models
  [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

Papers and codes for large vision-language models. 

This repo mainly focuses on the large vision-language models tasks. Please pull requests or email me by `yangcao.cs@gmail.com` if you want to recommend papers.  

If you are interested in related tasks, you can reach me out by discord account: yangcao#9724 or WeChat: 85298328912.

## 3D
1. <span id = "16001">[3D-LLM] [3D-LLM: Injecting the 3D World
into Large Language Models](https://arxiv.org/pdf/2307.12981.pdf), `NeurIPS2023`. [[Code](https://vis-www.cs.umass.edu/3dllm/)]
2. <span id = "16001">[LL3DA] [LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning](https://arxiv.org/pdf/2311.18651.pdf), `CVPR2024`. [[Code](https://github.com/Open3DA/LL3DA)]
3. <span id = "16001">[GPT4Point] [GPT4Point: A Unified Framework for Point-Language Understanding and Generation](https://arxiv.org/pdf/2312.02980), `CVPR2024`. [[Code](https://github.com/Pointcept/GPT4Point)]
4. <span id = "16001">[Uni3D] [Uni3D: Exploring Unified 3D Representation at Scale](https://arxiv.org/pdf/2310.06773), `ICLR2024`. [[Code](https://github.com/baaivision/Uni3D)]

## 2D
1. <span id = "16001">[LLaMA-VID] [LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models](https://arxiv.org/abs/2311.17043), `Arxiv2023`. [[Code](https://github.com/dvlab-research/LLaMA-VID)]
2. <span id = "16001">[Mini-Gemini] [Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models](https://arxiv.org/abs/2403.18814), `Arxiv2024`. [[Code](https://github.com/dvlab-research/MiniGemini)]
